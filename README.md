# Streamlit chat assistant

This is a streamlit app that will run LLM models locally using Ollama. You need to have Ollama installed. The app will offer a dropdownd list where you can choose the models you have initially pulled.

* You wan download Ollama models from: https://ollama.com/library
* You can see the Ollama documentation here: https://github.com/ollama/ollama

