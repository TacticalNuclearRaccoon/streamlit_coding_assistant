# Streamlit chat assistant

This is a streamlit app that will run LLM models locally using Ollama. You need to have Ollama installed. You need to have at least one model pulled from Ollama (I recommend starting with a samll one).

You can go the the [Ollama website](https://ollama.com/) to see installing instructions.

To pull and run a model (for example mistral):

$`ollama run mistral`

The catalog for all the models is [here](https://ollama.com/search)

The app will offer a dropdown list where you can choose the models you have initially pulled. Feel free to ask me for the docker image :) 

* You wan download Ollama models from: https://ollama.com/library
* You can see the Ollama documentation here: https://github.com/ollama/ollama

